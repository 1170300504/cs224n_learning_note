# Tranditional优化方法

### Batch gradient descent

$\theta = \theta - \eta \cdot \nabla_\theta J( \theta)$

### SGD

$\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})$

### Mini-batch gradient descent

$\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})$



## Challenges to above methods

Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:

- Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.
- Learning rate schedules [[1\]](http://ruder.io/optimizing-gradient-descent/index.html#fn1) try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [[2\]](http://ruder.io/optimizing-gradient-descent/index.html#fn2).
- Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.
- Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. [[3\]](http://ruder.io/optimizing-gradient-descent/index.html#fn3) argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.

## Solution

### Momentum

动量，简单来说就是让上一个时刻的影响保留，可以在和上一次方向一致的方向上进行动量累积，且可以抑制震荡（oscillations）

$\begin{align}  \begin{split}  \Delta \theta_t &= \gamma m_{t-1} + \eta \nabla_\theta J( \theta_t) \\  \theta_{t+1} &= \theta_t + \Delta \theta_t\end{split}  \end{align}$

### Nesterov accelerated gradient

基于Momentum，我们希望我们的优化可以有预见性，是基于预测结果而作出的优化决策。如下所示

$\begin{align}  \begin{split}  \Delta \theta_t &= \gamma m_{t-1} + \eta \nabla_\theta J( \theta_t - \gamma m_{t-1} ) \\  \theta_{t+1} &= \theta_t + \Delta \theta_t  \end{split}  \end{align}$

### Adagrad

针对参数调整了学习率。对于频繁出现的特征，学习率更低（通过分母大小来控制），不频繁出现的，学习率会更高。所以对稀疏sparse-data的效果更好。应用场景有youtube的cat recognition以及Glove word embedding的训练（as infrequent words require much larger updates than frequent ones.）。

$g_{t, i} = \nabla_\theta J( \theta_{t, i} )$

$\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}$

$\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}$

As GtGt contains the sum of the squares of the past gradients w.r.t. to all parameters θalong its diagonal, we can now vectorize our implementation by performing a matrix-vector product ⊙ between GtGt and gtgt:

$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}$

### Adadelta

Adagrad有这样的问题，如果gt累积太大，那么就会导致学习率过低，最后就不更新了。所以这里做了优化，只选择固定的过去w个g作为累积梯度

$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t$

所以按照Adagrad就有：

$\Delta \theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}$

As the denominator is just the root mean squared (RMS) error criterion of the gradient, we can replace it with the criterion short-hand:

$\Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t$

Like what we have done to g, we could have:

$E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t$

$RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}$

Therefore, the final format is :
$\begin{align}  \begin{split}  \Delta \theta_t &= - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t} \\  \theta_{t+1} &= \theta_t + \Delta \theta_t  \end{split}  \end{align}$

NOTE：为什么这里要对分子也做替换，其实是因为单位不统一。我们更新的都是theta，理论上应该是带单位的，但是旧版本的并没有带单位，【其实SGD什么的都没有管单位，只是直接给删除掉单位进行处理的】，所以这儿为了做到unit 的match做了这样的处理

### RMSprop

就是同一时间不同人提出的同一种方法。是大神提出的，Geoff Hinton， [Lecture 6e of his Coursera Class](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)。没有做发表，只是在课程上讲了。不过只有这个版本：

$\Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t$

### Adam

Adaptive Moment Estimation(Adam，自适应矩估计)，它也是一种可以自动调节不同参数学习率的算法，和Adadelta类似，还包含了类似Momentum保存之前累积的梯度的做法，这样就让Adam可以既更新方向，又自动更新学习率。它保存了参数的梯度累积，以及梯度平方的累积，在根据他们的期望估计新的参数在哪里。

我们来看是如何将他们结合起来的，首先有momentum：

$\begin{align}  \begin{split}  \Delta \theta_t &= \gamma m_{t-1} + \eta \nabla_\theta J( \theta_t) \\  \theta_{t+1} &= \theta_t + \Delta \theta_t\end{split}  \end{align}$

在这里，动量单独抽出来计算，把$\eta$抽出来，放在$\Delta \theta_t$前面，就有以下公式：

$m_t = \beta_1 m_{t-1} + (1 - \beta_1)  \nabla_\theta J( \theta_t)$

$\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1}$

就有

$\Delta \theta_t  = - {\eta} \hat{m}_t$

然后按照之前的RMSprop的方法，对学习率做decay：

$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t$

为了方便表示，替换为vt

$ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 $ 

$\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2} $

所以有，分母具体如何做平滑不重要，写进根号或者不写都ok：

$\Delta \theta_t  = - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$

最后有如下结果：

$\theta_{t+1} = \theta_{t} + \Delta \theta_t$



#### 后面还有三个，我懒得写了。。。

AdaMax
Nadam
AMSGrad

http://ruder.io/optimizing-gradient-descent/index.html#fn13